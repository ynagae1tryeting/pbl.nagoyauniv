# -------------------------------------------------------
# 講義用(2)
# 旅客機利用者数の推移の時系列データの前処理を行った上で実施する
# caretパッケージを用いた上での機械学習メソッド
# -------------------------------------------------------

# -------------------------------------------------------
# 原系列
# Rに組み込んであるデータ系列
# -------------------------------------------------------
# 今回は毎月の飛行機乗客数のデータを使います。
# この場合「先月の乗客数が多ければ、来月も多くなるのではないか」といったように、過去から未来を予測していきます。
# 時間という情報を基にして将来を予測する方法を学んでください。
# -------------------------------------------------------
AirPassengers

# -------------------------------------------------------
# 原系列に対して、対数差分系列を作ります。
## 対数差分系列への適用
# -------------------------------------------------------
log_diff_passenger <- diff(log(AirPassengers))
log_diff_passenger

# 原系列と対数差分系列のグラフを描いておきます。
par(mfrow=c(2,1))
plot(AirPassengers, main="Origin")
plot(log_diff_passenger, main="diff_log")
par(mfrow=c(1,1))

# -------------------------------------------------------
# 差分系列とは、例えば｛４，７，５｝というデータがあった時｛７－４，５－７｝＝｛３，－２｝というようにスライドして差をとった結果のことです。
# あらかじめ対数をとることにより、データを計算しやすい幅に収めやすくなる、また、
# 対数差分系列は近似的に「変化率」の推移と等しくなるので解釈がしやすくなるというメリットが得られます。
# なお、差分をとった時に、最初のデータが（差分を計算できないので）無くなってしまうことに注意してください。

# 元に戻すには、差分系列の累積和を計算してから、「一番最初の値」を足してやればよいです。
# 先の例ですと、差分系列が｛３，－２｝でしたので、最初の値をいれて｛４，３，－２｝、累積和をとって｛４，４＋３，４＋３－２｝＝｛４，７，５｝に戻ります。
# Rで確認してみます。
# -------------------------------------------------------

# 元に戻す
exp(cumsum(log_diff_passenger) + log(AirPassengers[1]))

# 参考
as.numeric(AirPassengers)[-1]


# -------------------------------------------------------
# データをもう少し変換します。
# 今度はデータの中身は変えません。
# 機械学習法を適用しやすいように整形します。
# ラグをとって、過去のデータを説明変数にする
# embed関数による整形(http://d.hatena.ne.jp/teramonagi/20121002/1349183342)
# -------------------------------------------------------

lag_num <- 4
exp_val_sample <- as.data.frame(embed(log_diff_passenger, lag_num))

# 列名の変更
colnames(exp_val_sample)[1] <- "Y"
for(i in 2:lag_num){
  colnames(exp_val_sample)[i] <- paste("Lag", i-1, sep="")
}

# 整形されたデータ
head(exp_val_sample)

# こんな感じになるはずやで
#             Y        Lag1        Lag2        Lag3
# 1 -0.06402186 -0.02298952  0.11211730  0.05218575
# 2  0.10948423 -0.06402186 -0.02298952  0.11211730
# 3  0.09193750  0.10948423 -0.06402186 -0.02298952
# 4  0.00000000  0.09193750  0.10948423 -0.06402186
# 5 -0.08455739  0.00000000  0.09193750  0.10948423
# 6 -0.13353139 -0.08455739  0.00000000  0.09193750


# -------------------------------------------------------
# 機械学習による時系列予測
# -------------------------------------------------------
## caretによる学習モデル作成の準備
# install.packages("kernlab")
# install.packages("caret")
# install.packages("e1071")
library(caret)
library(kernlab)

# 並列化演算を行う
# install.packages("doParallel")
library(doParallel)
cl <- makePSOCKcluster(detectCores())
registerDoParallel(cl)

# -------------------------------------------------------
# Rによる機械学習(train関数)
# 1行目：Y（直近のデータ）を、他のデータ（過去のデータ）すべてを使って予測します
# 2行目：学習データの指定
# 3行目：予測モデルの指定。ガウシアンカーネルを使ったSVMを使用します
# 4行目：ハイパーパラメタをどの範囲で動かすかの指定。
# 5行目：データをあらかじめ標準化してね、という指定
# -------------------------------------------------------
# モデルの推定
set.seed(0)
tuned_svm_sample <- train(
  Y ~ ., 
  data=exp_val_sample,
  method = "svmRadial", 
  tuneGrid = expand.grid(C=c(1:5), sigma=2^c(-1:1)),
  preProcess = c('center', 'scale')
)

# チューニングされたモデル(モデルオブジェクト)
tuned_svm_sample

# 最小のRMSE（予測誤差）
tuned_svm_sample$results$RMSE
min(tuned_svm_sample$results$RMSE)

# チューニングされたハイパーパラメタ
tuned_svm_sample$bestTune

# SVMのモデルだけを取り出す
tuned_svm_sample$finalModel


# -------------------------------------------------------
# 最適な次数を選ぶ
# -------------------------------------------------------
# モデルの作り方がわかったので、次は最適な次数を選ぶ方法を説明します。
# 次数とは、例えば「前月データだけを使って予測する」なら次数は１となります。
# 2か月前までのデータを使うなら次数は２です。
# 何か月前までのデータを使うと最も予測精度が高くなるのか。それをこれから調べるということです。

# 予測精度などを格納する入れ物
sim_result <- data.frame(
  order=numeric(), 
  error=numeric(),
  C=numeric(),
  sigma=numeric()
)

# 予測モデルの一覧を格納する入れ物
tuned_models <- list()


# 次に、最大ラグ数を指定して、データを整形します。
# 今回は12を最大ラグ数としました。MAXで12か月前までのデータを使って予測するモデルを作るということです。
# なお、直近のデータだけテスト用に残しておきました。

# 最大ラグ数
lag_max <- 12

# 訓練データの作成
# 最後のデータだけテスト用に残しておく
exp_val <- as.data.frame(embed(log_diff_passenger[-length(log_diff_passenger)], lag_max + 1))
colnames(exp_val)[1] <- "Y"
for(i in 2:(lag_max+1)){
  colnames(exp_val)[i] <- paste("Lag", i-1, sep="")
}

# 訓練データ表示
head(exp_val)

# 　こうやって表示されるはず
# Y        Lag1        Lag2        Lag3        Lag4        Lag5        Lag6        Lag7
# 1  0.09134978 -0.02575250  0.12629373 -0.13473259 -0.13353139 -0.08455739  0.00000000  0.09193750
# 2  0.11247798  0.09134978 -0.02575250  0.12629373 -0.13473259 -0.13353139 -0.08455739  0.00000000
# 3 -0.04348511  0.11247798  0.09134978 -0.02575250  0.12629373 -0.13473259 -0.13353139 -0.08455739
# 4 -0.07696104 -0.04348511  0.11247798  0.09134978 -0.02575250  0.12629373 -0.13473259 -0.13353139
# 5  0.17563257 -0.07696104 -0.04348511  0.11247798  0.09134978 -0.02575250  0.12629373 -0.13473259
# 6  0.13185213  0.17563257 -0.07696104 -0.04348511  0.11247798  0.09134978 -0.02575250  0.12629373
#         Lag8        Lag9       Lag10       Lag11       Lag12
# 1  0.10948423 -0.06402186 -0.02298952  0.11211730  0.05218575
# 2  0.09193750  0.10948423 -0.06402186 -0.02298952  0.11211730
# 3  0.00000000  0.09193750  0.10948423 -0.06402186 -0.02298952
# 4 -0.08455739  0.00000000  0.09193750  0.10948423 -0.06402186
# 5 -0.13353139 -0.08455739  0.00000000  0.09193750  0.10948423
# 6 -0.13473259 -0.13353139 -0.08455739  0.00000000  0.09193750


# 最後に、次数を変えながら計算を繰り返し、その都度予測モデルと予測精度を保存しておきます。
# 計算には若干の時間がかかることに注意してください。
# 時間がかかるため、パラメタのチューニングはかなり荒くしてあります。

# ループさせて、最も予測精度が高くなるラグ数を調べる
set.seed(0)
for(i in 1:lag_max) {

  # 必要なラグまで、データを切り落とす
  exp_val_tmp <- exp_val[,c(1:(i+1))]
  
  # 予測モデルの作成
  tuned_svm <- train(
    Y ~ ., 
    data=exp_val_tmp,
    method = "svmRadial", 
    tuneGrid = expand.grid(C=c(1:3), sigma=2^c(-1:1)),
    preProcess = c('center', 'scale')
  )
  
  # 予測モデルを保存する
  tuned_models <- c(tuned_models, list(tuned_svm))
  
  # 予測精度などを保存する
  sim_result[i, "order"] <- i
  sim_result[i, "error"] <- min(tuned_svm$results$RMSE)
  sim_result[i, "sigma"] <- tuned_svm$bestTune["sigma"]
  sim_result[i, "C"]     <- tuned_svm$bestTune["C"]
  
}

# 次数と予測精度の関係を図示してみる。
# ラグと予測精度の関係
plot(
  sim_result$error ~ sim_result$order,
  main="lag-order vs accuracy",
  xlab="lag-order",
  ylab="error"
)

# -------------------------------------------------------
# 当てはめ精度の評価
# -------------------------------------------------------
# 次に、予測精度や最も予測精度の高かったモデルを取り出します。

# 最も予測精度が高かったモデルの評価結果
subset(sim_result, sim_result$error==min(sim_result$error))

# 次数だけを取り出す
best_order <- subset(sim_result, sim_result$error==min(sim_result$error))$order
best_order

## 当てはまりの精度の確認
# 最も予測精度が高かったモデルを取り出す
best_model <- tuned_models[[best_order]]
best_model

# 予測値と実測値のプロット
plot(log_diff_passenger[-c(1:lag_max, length(log_diff_passenger))], type="l")
lines(predict(best_model), col=2, lwd=2, lty=2)
legend(
  legend=c("実測値", "予測値"),
  "topright",
  col=c(1,2),
  lwd=c(1,2),
  lty=c(1,2)
)

## 1期先の将来予測

# 説明変数を作る
# 直近のデータだけを抽出
last_data <- exp_val[nrow(exp_val), 1:best_order]

# 列名を、学習データに合わせる
for(i in 1:best_order){
  colnames(last_data)[i] <- paste("Lag", i, sep="")
}

# 説明変数を表示
last_data

# 1期先の予測
predict(best_model, last_data)
# [1] 0.07098058

# 正解データと比較
log_diff_passenger[length(log_diff_passenger)]
# [1] 0.1022788

